{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Pokemon_Classify.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/syedmujahedalih/ForskLabs_DL/blob/master/CNN_Pokemon_Classify_pretrainedVGG.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "QkcFgSxJFymW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1096
        },
        "outputId": "1c336494-0be5-41a8-d5f4-bc391df7ea86"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "#!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "#!apt-get update -qq 2>&1 > /dev/null\n",
        "#!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!apt-get install -f\n",
        "!apt-get -y install -qq fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "--2018-10-17 12:49:25--  https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
            "Resolving launchpad.net (launchpad.net)... 91.189.89.223, 91.189.89.222\n",
            "Connecting to launchpad.net (launchpad.net)|91.189.89.223|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb [following]\n",
            "--2018-10-17 12:49:26--  https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
            "Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 91.189.89.229, 91.189.89.228\n",
            "Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|91.189.89.229|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1232624 (1.2M) [application/x-debian-package]\n",
            "Saving to: ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb’\n",
            "\n",
            "google-drive-ocamlf 100%[===================>]   1.17M   777KB/s    in 1.5s    \n",
            "\n",
            "2018-10-17 12:49:29 (777 KB/s) - ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb’ saved [1232624/1232624]\n",
            "\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 22278 files and directories currently installed.)\n",
            "Preparing to unpack google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-drive-ocamlfuse:\n",
            " google-drive-ocamlfuse depends on libfuse2 (>= 2.8); however:\n",
            "  Package libfuse2 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-drive-ocamlfuse (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Errors were encountered while processing:\n",
            " google-drive-ocamlfuse\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libfuse2\n",
            "Suggested packages:\n",
            "  fuse\n",
            "The following NEW packages will be installed:\n",
            "  libfuse2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 80.9 kB of archives.\n",
            "After this operation, 313 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfuse2 amd64 2.9.7-1ubuntu1 [80.9 kB]\n",
            "Fetched 80.9 kB in 1s (89.0 kB/s)\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 22283 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "(Reading database ... 22295 files and directories currently installed.)\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrQesiAEebgx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPvNB0eYfWRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SsQGX3EfbIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "571a7eb6-4bbd-4d6f-b6f0-f21673e061cc"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fc_ac8Rkflbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0U9QgyYnf9HB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpSlm4ZkgKkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Pooling\n",
        "\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YdQeDiOPgV5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2So2sNHHgY1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ChgiSrZgeDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 3, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjnY4Urygno_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g004wfhsg1cN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "615d72c4-1e72-4a7e-ef11-87c5366f357e"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the test data shouldn’t be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('drive/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('drive/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 445 images belonging to 3 classes.\n",
            "Found 238 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VtiF5bL5haHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f44dee2c-cf27-4da7-a5ef-070ca2061140"
      },
      "cell_type": "code",
      "source": [
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 100,\n",
        "                         epochs = 5,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  8/100 [=>............................] - ETA: 2:35 - loss: 1.1741 - acc: 0.4609"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 256s 3s/step - loss: 0.3561 - acc: 0.8608 - val_loss: 0.2240 - val_acc: 0.9249\n",
            "Epoch 2/5\n",
            " 99/100 [============================>.] - ETA: 1s - loss: 0.0963 - acc: 0.9621"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-XAhTTgI0O6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Using the pretrained model: VGG16**"
      ]
    },
    {
      "metadata": {
        "id": "CMDI67H90YJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KytAQUd0nx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8cbdc034-2e81-4fa3-8694-5dff73056ad4"
      },
      "cell_type": "code",
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "include_top=False,\n",
        "input_shape=(150, 150, 3))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gCvUus990t1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iAr5uWvx01z6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False #do not train the conv-base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1Vuqzci1R4-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7GyQnk21FHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_dir = 'drive/dataset/'\n",
        "train_dir = os.path.join(base_dir, 'training_set')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test_set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSsQZ5361JKj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2yRFcOV1fdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "215857b8-d517-4a84-c68a-084409fc4738"
      },
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 445 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N_EcJ8QA1i4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48e1fb7d-ef70-490f-e209-c27667568aca"
      },
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(150, 150),\n",
        "                                                  batch_size=20,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 238 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JWzxEH8W2DCl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ok9fDVf82b5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "b5eb2695-78a4-4da6-80fb-60fc67179725"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=100,\n",
        "                              epochs=15,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " 15/100 [===>..........................] - ETA: 20:45 - loss: 1.0093 - acc: 0.5500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 596s 6s/step - loss: 0.7098 - acc: 0.7675 - val_loss: 0.5011 - val_acc: 0.8599\n",
            "Epoch 2/15\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.3874 - acc: 0.9060 - val_loss: 0.3502 - val_acc: 0.8679\n",
            "Epoch 3/15\n",
            "100/100 [==============================] - 121s 1s/step - loss: 0.2673 - acc: 0.9285 - val_loss: 0.3073 - val_acc: 0.8851\n",
            "Epoch 4/15\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.2158 - acc: 0.9375 - val_loss: 0.2656 - val_acc: 0.8992\n",
            "Epoch 5/15\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.1757 - acc: 0.9545 - val_loss: 0.2337 - val_acc: 0.9214\n",
            "Epoch 6/15\n",
            "100/100 [==============================] - 121s 1s/step - loss: 0.1492 - acc: 0.9635 - val_loss: 0.2012 - val_acc: 0.9224\n",
            "Epoch 7/15\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.1320 - acc: 0.9630 - val_loss: 0.1947 - val_acc: 0.9163\n",
            "Epoch 8/15\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.1173 - acc: 0.9725 - val_loss: 0.1920 - val_acc: 0.9234\n",
            "Epoch 9/15\n",
            "100/100 [==============================] - 123s 1s/step - loss: 0.1086 - acc: 0.9725 - val_loss: 0.1970 - val_acc: 0.9224\n",
            "Epoch 10/15\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0976 - acc: 0.9790 - val_loss: 0.1860 - val_acc: 0.9294\n",
            "Epoch 11/15\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.0945 - acc: 0.9705 - val_loss: 0.1693 - val_acc: 0.9254\n",
            "Epoch 12/15\n",
            "100/100 [==============================] - 124s 1s/step - loss: 0.0873 - acc: 0.9770 - val_loss: 0.1729 - val_acc: 0.9244\n",
            "Epoch 13/15\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.0779 - acc: 0.9810 - val_loss: 0.1606 - val_acc: 0.9294\n",
            "Epoch 14/15\n",
            "100/100 [==============================] - 115s 1s/step - loss: 0.0698 - acc: 0.9820 - val_loss: 0.1610 - val_acc: 0.9304\n",
            "Epoch 15/15\n",
            "100/100 [==============================] - 123s 1s/step - loss: 0.0732 - acc: 0.9835 - val_loss: 0.1465 - val_acc: 0.9355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P4Cc1LmWRE8P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "    \n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZDP_zw1TSVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1114
        },
        "outputId": "3ad1b668-f578-4fd1-83d6-a23643d21da7"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 16/100 [===>..........................] - ETA: 1:14 - loss: 0.0629 - acc: 0.9844"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:872: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 129s 1s/step - loss: 0.0327 - acc: 0.9897 - val_loss: 0.0527 - val_acc: 0.9698\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.0620 - val_acc: 0.9731\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 122s 1s/step - loss: 0.0138 - acc: 0.9962 - val_loss: 0.0428 - val_acc: 0.9798\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 120s 1s/step - loss: 0.0051 - acc: 0.9978 - val_loss: 0.0781 - val_acc: 0.9724\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.0591 - val_acc: 0.9805\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 123s 1s/step - loss: 0.0041 - acc: 0.9993 - val_loss: 0.0563 - val_acc: 0.9765\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0374 - val_acc: 0.9738\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 115s 1s/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.1066 - val_acc: 0.9644\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 124s 1s/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0247 - val_acc: 0.9892\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0977 - val_acc: 0.9651\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0486 - val_acc: 0.9812\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 127s 1s/step - loss: 8.9192e-04 - acc: 0.9997 - val_loss: 0.0544 - val_acc: 0.9772\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 116s 1s/step - loss: 9.7848e-04 - acc: 0.9997 - val_loss: 0.0253 - val_acc: 0.9805\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0242 - val_acc: 0.9892\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 126s 1s/step - loss: 3.0140e-04 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9718\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0662 - val_acc: 0.9778\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 118s 1s/step - loss: 6.7977e-04 - acc: 0.9997 - val_loss: 0.0300 - val_acc: 0.9866\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 128s 1s/step - loss: 7.4704e-04 - acc: 0.9997 - val_loss: 0.1270 - val_acc: 0.9718\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 0.0010 - acc: 0.9993 - val_loss: 0.0542 - val_acc: 0.9677\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 8.6616e-05 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9879\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 129s 1s/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0588 - val_acc: 0.9745\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 117s 1s/step - loss: 8.9647e-04 - acc: 0.9993 - val_loss: 0.0504 - val_acc: 0.9798\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 124s 1s/step - loss: 3.3764e-05 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 0.9798\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 3.7287e-04 - acc: 0.9997 - val_loss: 0.0233 - val_acc: 0.9812\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 4.3480e-05 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.9751\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 127s 1s/step - loss: 6.8666e-04 - acc: 0.9997 - val_loss: 0.0584 - val_acc: 0.9684\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 119s 1s/step - loss: 8.0091e-04 - acc: 0.9997 - val_loss: 0.1293 - val_acc: 0.9698\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 116s 1s/step - loss: 4.7733e-04 - acc: 0.9997 - val_loss: 0.1061 - val_acc: 0.9644\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 125s 1s/step - loss: 2.6662e-04 - acc: 1.0000 - val_loss: 0.0316 - val_acc: 0.9819\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 122s 1s/step - loss: 9.6742e-04 - acc: 0.9997 - val_loss: 0.0311 - val_acc: 0.9785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fya_9FvCTau6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f80dec00-b2b3-49c1-d48d-15ebd0aa2ad1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('drive/dataset/single_prediction/pokemon1.png', target_size = (150, 150, 3))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model.predict(test_image)\n",
        "training_set = train_datagen.flow_from_directory('drive/dataset/training_set',\n",
        "                                                 target_size = (64,64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "training_set.class_indices"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 445 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bulbasaur': 0, 'charmander': 1, 'pikachu': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "GffBF9UrhvPg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}